# PyTorch-binary-classification-Mobile-Net
![Image text](https://github.com/futureisatyourhand/PyTorch-binary-classification-Mobile-Net/blob/master/losses.png)
# AVG->1024
model_10.pth acc:0.5467128027681661

model_95.pth acc:0.5951557093425606

model_40.pth acc:0.6238260009886307

model_45.pth acc:0.6188828472565496

model_35.pth acc:0.6080079090459714

model_70.pth acc:0.6213544241225902

model_85.pth acc:0.5921898171033119

model_30.pth acc:0.5981216015818092

model_0.pth acc:0.5170538803756797

model_75.pth acc:0.5981216015818092

model_20.pth acc:0.5748887790410282

model_55.pth acc:0.6154226396440929

model_90.pth acc:0.6154226396440929

model_5.pth acc:0.532377656945131

model_25.pth acc:0.5773603559070687

model_60.pth acc:0.6124567474048442

model_50.pth acc:0.6188828472565496

model_65.pth acc:0.620365793376174

model_80.pth acc:0.6119624320316361

We find that the model is over-fitting for binary classification. Therefore, I update the channels of final convolution layer to 10.
# avg->10
model_10.pth acc:0.7839841819080573

model_95.pth acc:0.8403361344537815

model_40.pth acc:0.8442906574394463

model_45.pth acc:0.8383588729609491

model_35.pth acc:0.8329214038556599

model_70.pth acc:0.8398418190805734

model_85.pth acc:0.837864557587741

model_30.pth acc:0.8294611962432031

model_0.pth acc:0.597132970835393

model_75.pth acc:0.8546712802768166

model_20.pth acc:0.8329214038556599

model_55.pth acc:0.833415719228868

model_90.pth acc:0.8408304498269896

model_5.pth acc:0.7251606524962926

model_25.pth acc:0.8413247652001977

model_60.pth acc:0.8373702422145328

model_50.pth acc:0.8482451804251112

model_65.pth acc:0.8373702422145328

model_80.pth acc:0.8398418190805734

model_15.pth acc:0.8067226890756303

model_15.pth acc:0.5541275333662877
